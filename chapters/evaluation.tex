\chapter{Evaluation}
\label{ch:evaluation}

This chapter first introduces the setup that was used for evaluating the different elasticity strategies. Then the results of different tests are presented and discussed.

\section{Testsetup}
\label{sec:testsetup}

In order to test the different elasticity strategies a test environment has to be set up. It was decided to create three virtual machines (VM) that will form a Kubernetes cluster. Because of its easy of use microk8s was chosen as distribution\footnote{\url{https://microk8s.io/}}. All three virtual machines were assigned 10 vCPUs and 10GB of memory. One VM acts as the Kubernetes control plane while the other two join the cluster as worker nodes.

Everything that was deployed into the Kubernetes cluster was built using the infrastructure as code (IaC) tool HashiCorp Terraform\footnote{\url{https://www.terraform.io/}}. This enables rapid changes and reproducibility. Deployed resources include the kube-prometheus-stack\footnote{\raggedright\url{https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack}} for monitoring, the k8ssandra-operator\footnote{\url{https://docs.k8ssandra.io/components/k8ssandra-operator/}} for managing k8ssandra clusters and a definition for a k8ssandra cluster. Additionally, the in \cref{sec:metrics} mentioned Grafana dashboards are also deployed using Terraform.

\Cref{lst:k8c} illustrates a minimal definition of a 3 node k8ssandra cluster. Each node has resource limits of 800 millicpu and 6000MB of memory and 3GiB storage space.

\begin{lstlisting}[caption={Minimal example of a K8ssandraCluster definition.},
                label=lst:k8c,
                captionpos=b,
                float]
apiVersion: k8ssandra.io/v1alpha1
kind: K8ssandraCluster
metadata:
  name: polaris-test-cluster
  namespace: k8ssandra
spec:
  cassandra:
    resources:
      limits:
        cpu: 800m
        memory: 6000M
    datacenters:
      - metadata:
          name: dc1
        size: 3
        storageConfig:
          cassandraDataVolumeClaimSpec:
            resources:
              requests:
                storage: 3Gi
\end{lstlisting}

\section{Benchmarks}

In the following sections, different test scenarios will be discussed. To let k8ssandra experience load, the built-in stress testing tool \texttt{cassandra-stress} was used\footnote{\raggedright\url{https://cassandra.apache.org/doc/stable/cassandra/tools/cassandra_stress.html}}.

\subsection{Stress Testing}
\label{sec:stress-testing}

To set a baseline, three different k8ssandra cluster setups have been stress tested using \texttt{cassandra-stress}. The amount of write requests that the tool will make is set to be 1000000, the exact call is listed in \cref{lst:stress-1000000writes}. These cluster setups merely differ in the cluster size, thus the amount of nodes. All clusters were provisioned with limits of 2 CPUs and 6GB of memory.

\begin{lstlisting}[caption={},
                    captionpos=b,
                    label=lst:stress-1000000writes,
                    float]
./cassandra-stress write n=1000000 -mode native cql3 \
    user='USERNAME' password='PASSWORD'
\end{lstlisting}

The results of these tests are depicted in \cref{fig:stress-1000000writes-1node,fig:stress-1000000writes-2node,,fig:stress-1000000writes-3node}. The write throughput increases with the amount of nodes, but not linearly. This, however, was to be expected as \texttt{cassandra-stress} does not partition data in way that favours linear scalability. The average write throughputs of these different clusters can be seen in \cref{tab:stress-1000000writes-ops}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Cluster size} & \textbf{operations/s} & \textbf{Time to complete} \\ \hline
1                     & 12514                 & 2m38s                     \\ \hline
2                     & 13142                 & 1m57s                     \\ \hline
3                     & 14318                 & 1m50s                     \\ \hline
\end{tabular}
\caption{Average write throughput for different k8ssandra clusters. With increasing cluster size the throughput also increases}
\label{tab:stress-1000000writes-ops}
\end{table}

\begin{figure}
    \centering
    \input{figures/plots/stress-1000000writes-1node.pgf}
    \caption{Stress test of 1 node with 1000000 writes}
    \label{fig:stress-1000000writes-1node}
\end{figure}

\begin{figure}
    \centering
    \input{figures/plots/stress-1000000writes-2node.pgf}
    \caption{Stress test of 2 nodes with 1000000 writes}
    \label{fig:stress-1000000writes-2node}
\end{figure}

\begin{figure}
    \centering
    \input{figures/plots/stress-1000000writes-3node.pgf}
    \caption{Stress test of 3 nodes with 1000000 writes}
    \label{fig:stress-1000000writes-3node}
\end{figure}

\subsection{Vertical Elasticity Strategy}
\label{sec:evaluation-vertical-elasticity}

As mentioned in \cref{sec:vertical-elasticity} the vertical elasticity strategy adjusts the resource claims of k8ssandra according to its CPU and memory utilization.

As it can bee seen in \cref{fig:simple-limits-vertical} the elasticity strategy controller successfully changes the CPU and memory limits of the k8ssandra cluster once it is operational. \Cref{fig:utilization-vertical} shows the CPU and memory utilization that is used for triggering elasticity processes. Because the CPU utilization stays very low even after scaling takes place, it can be assumed that this metric was not a decisive factor. The memory utilization, however, changes notably. Before starting the elasticity strategy controller the actual memory utilization was off by \(>10\%\) from the target memory utilization. This triggers an elasticity event and the resources are adjusted proportionally.

Interestingly, during reconsiliation the exposed metrics of k8ssandra are not very meaningful. During this process utilization values of far more than 100\% are exposed by the metrics controller. In order to keep the diagram clean, these nonsense-metrics have been filtered out. The reconsiliation process is marked red in \cref{fig:utilization-vertical}.

This elasticity strategy mirrors real-life scenarios. The advantage lies in being able to scale down when demand and therefore CPU and memory utilization is low, thus potentially reducing cost. This obviously only applies when not using dedicated resources.

\begin{figure}
    \centering
    \input{figures/plots/simple-limits-vertical-controller-1node-30min.pgf}
    \caption{Adjustment of CPU and memory limits by the vertical elasticity strategy controller}
    \label{fig:simple-limits-vertical}
\end{figure}

\begin{figure}
    \centering
    \input{figures/plots/utilization-vertical-controller-1node-20min.pgf}
    \caption{Utilization of CPU and memory during an vertical scaling action}
    \label{fig:utilization-vertical}
\end{figure}

\subsection{Horizontal Elasticity Strategy}
\label{sec:evaluation-horizontal-elasticity}

The horizontal elasticity strategy controller scales the target k8ssandra cluster horizontally, thus adding nodes as demand increases. Demand is measured as write throughput by the metrics controller as described in \cref{sec:metrics-average-write-utilization}.

As in the example stress tests discussed in \cref{sec:stress-testing}, \texttt{cassandra-stress} was used to generate load on the target k8ssandra cluster. During this load generation process, the horizontal elasticity controller was running. The target write load per node was defined in the SLO mapping as 5000. Depicted in \cref{fig:horizontal-elasticity} is the average write load per node metric and the corresponding node count during the testing process. It can be seen that the node count does not increase immediatly when the scaling action takes place. That is because when the k8ssandra CRD is updated by the elasticity strategy controller, first the \texttt{k8ssandra-operator} has to recognize the made changes and adjust the configuration accordingly. When the second k8ssandra node is successfully scheduled it still needs time to start and finally register in the cluster. The final action is the Cassandra reconciliation process.

At approximately 290s a sudden drop in the metric can be observed. This is the point when the scaling action becomes effective and the k8ssandra node is ready. Then, after another few moments the metric drops under the set boundary of 5000. Tests of this kind are difficult to run over an extended period of time because of a limitation of \texttt{cassandra-stress}. When the load generator is started, it collects all available nodes in the cluster through Cassandra's communication protocol \texttt{Gossip}. \texttt{Gossip} is the protocol that Cassandra uses internally for its nodes to communicate with each other\footnote{\raggedright\url{https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/architecture/archGossipAbout.html}}. While \texttt{cassandra-stress} is running, new nodes are not recognized and requests are therefore not sent to added nodes. Possible solutions to this will be discussed in \cref{ch:conclusion}.

\begin{figure}
    \centering
    \input{figures/plots/horizontal-elasticity.pgf}
    \caption{Average write load per node and amount of nodes during a horizontal scaling action}
    \label{fig:horizontal-elasticity}
\end{figure}

\subsection{Diagonal Elasticity Strategy}

As explained earlier, the diagonal elasticity strategy combines the capabilites of the vertical and horizontal elasticity strategy into one single elasticity strategy.

\Cref{fig:diagonal-elasticity} summarizes all metrics into a single illustration. The starting configuration was set to be a single k8ssandra node with resources of 2 CPUs and 6GB of memory. After starting the elasticity strategy controller it can be seen in \cref{fig:diagonal-elasticity-limits} that the controller immediatly reduces both CPU and memory resources. The reason for that can be seen in \cref{fig:diagonal-elasticity}, subfigure \texttt{c} and \texttt{d}. Right at the start, both CPU and memory utilization was not within the tolerance range of the target utilization. Therefore both CPU and memory limits were reduced. After the inital adjustment, the CPU utilization was still far away from the targeted amount. That is because the CPU resources hit the statically set lower bounds. The memory utilization however climbed above the targeted amount, therefore it was reduced again in the second taken scaling action.

Similarly to \cref{sec:evaluation-vertical-elasticity}, during Cassandra reconsiliation metrics are not very useful. This is again highlighted in red in \cref{fig:diagonal-elasticity}.

During the second scaling action it can be seen that vertical and horizontal scaling indeed can happen simultaneously. In subfigure \texttt{b} of \cref{fig:diagonal-elasticity} the node count increased to 2, whereas in \cref{fig:diagonal-elasticity-limits} the memory limits increased. Note, that the \texttt{k8ssandra-operator} adjusts those values one at a time. This means that first the second k8ssandra node is started and then both Pods will get its resources updated accordingly.

Horizontal scaling action are taken when the write load per node reaches a certain threshold, 5000 in this example. In \cref{fig:diagonal-elasticity}, subfigure \texttt{a} it can be seen that after the second, third and fourth scaling events, the k8ssandra cluster size is increas, thus an additional node is started. During the fifth and last scaling event no additional node is started, because the statically set maximum amount of nodes is reached. It is also visible, that the total write throughput increases with increasing node count. This can be further illustrated by multiplying the estimated peak write load with the current node count. \(18000 * 1 = 18000,\ 12000 * 2 = 24000,\ 9000 * 3 = 27000 \rightarrow 18000 < 24000 < 27000\).

Because of the in \cref{sec:evaluation-horizontal-elasticity} addressed drawback of \texttt{cassandra-stress}, which does not detect changes to the cluster architecture, stress tests were cancelled after new nodes were added, and restarted when Cassandra had finished its reconsiliation process.

The advantage of this elasticity strategy is its ability to scale vertically and horizontally independently. This means that during times of low demand resources can be saved or used by other applications. A lower amount of resources also implies lower costs. During high demand times resources can be claimed again to provide a sufficient service level. If k8ssandra reports a high amount of writes the elasticity strategy can the also decide to scale-out horizontally by adding more nodes. As it was shown in \cref{sec:stress-testing} this increases the total throughput. As mentioned before, horizontal scale-in is not implemented in this project. This will be further addressed in \cref{sec:future-work}.

\begin{figure}
    \centering
    \input{figures/plots/diagonal-elasticity.pgf}
    \caption{Adjustment of CPU and memory resources aswell as cluster size during diagonal elasticity}
    \label{fig:diagonal-elasticity}
\end{figure}

\begin{figure}
    \centering
    \input{figures/plots/diagonal-elasticity-limits.pgf}
    \caption{CPU and memory limits while the diagonal elasticity strategy controller is running}
    \label{fig:diagonal-elasticity-limits}
\end{figure}
